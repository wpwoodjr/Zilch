.lm 5
.rm 80
.sp 2
.nj
.skip
1.Introduction
.p5,1,1
The analysis of matched case-control studies via the logistic regression
model and extensions of it has become commonplace in epidemiology.  These
analyses raise important and troubling questions regarding the fit of the
statistical models and the stability of the parameter estimates.  More
precisely, we are interested in the following questions.  First, are there
individual observations or sets of observations that are badly fit by the
model?  Second, are there observations that unduly influence the fit of the
model in the sense that deletion of these observations greatly influence
the parameter estimates?
.p5,1,1
In linear regression, these questions have been extensively investigated.
A crucial construction in linear regression diagnostics is that of the hat
matrix, and important concepts are those of residual and leverage.  Consider
the linear regression problem, y#=#X##+###where y~t`#=#(y`1~,y`2~,...,y`n~) is an
n-vector of observations, #~t`#=#(#`1~,#`2~,...,#`d~) is a d-vector of parameters
and X is an n x d design matrix.  Then, it is well known that the vector of
fitted values y#=#X(X~t`X)~-1`X~t`Y.  Because of this relationship between y and
y, the matrix H#=#X(X~t`X)~-1`X~t` is called the hat matrix.  The k~th` diagonal
element of H, h`kk~, is called the leverage associated with the k~th` observation
y`k~.  Let###=#(X~t`X)~-1`X~t`y be the estimate of##, and suppose that##`(k)~ is the
estimate of###with the k~th` observation, y`k~, deleted.  Then,
.indent 15
#`k~#=###-##`(k)~#=#(X~t`X)~-1` x`k~r`k~/1-h`kk~,
where r`k~#=#y`k~#-#y`k~ is the k~th` component of the vector of residuals and x##is
the row of X corresponding to the k###observation.  Thus, both the fit as
measured by the residual r`k~ and the leverage are important in determining
#`k~.  Cook (1977) proposed labelling points as influential if the value of
the quadratic form
.rm +20
###############(##-##`(k)~)~t`#(X~t`X)#(##-##`(k)~)#=#r`k~_~2`h`kk~/(1-h`kk~)~2`
.rm -20
.page size ,80
is unusually large.
.p5,1,1
The main goal of this paper is to provide methods for evaluating the
impact of single observations (risk sets and individuals within risk sets)
on the parameter estimates of regression models for the analysis of matched
case-control data.  We consider both the logistic regression model, and
extensions of it, in the form of generalized relative risk functions (Thomas,
1981).  In particular, we will be concerned with how individual observations
affect the choice of a model when the mixture model suggested by Thomas (1981)
is used for the analysis.
.p5,1,1
Now, let##`(k)~ represent the parameter estimates in a matched case-control
study on deletion of the k~th` risk set.  For the regression models used in
case-control studies, exact formulae for###-##`(k)~ are not available, and
approximations must be employed.
.p5,1,1
In section 2, we re-express the conditional likelihood of a matched case-
control study in a form that is particularly suitable for diagnostics.  In a
recent paper, Pregibon (1981) has considered diagnostic statistics in the
context of logistic regression.  Pregibon's prescription for approximating
##-##`(k)~ is based on a single iteration of the Newton-Raphson algorithm, and
we review it in section 3 in the context of matched case-control data.  The
one-step Newton-Raphson approximation is computationally difficult to
implement with generalized relative risk and mixture models, and we introduce
another, computationally simpler, approach in section 4.  Our approach is
identical to Pregibon's for logistic regression.  In section 4, we also
introduce the appropriate notions of hat matrix, residual, leverage and
influence.  Finally, in section 5, we present a detailed example.
.s
2.  The Matched Case-Control Study:
.p5,1,1
Suppose that there are n cases, and that for the i~th` case, there are m`i`
controls.  Let R`i~ denote the i~th` risk set, that is, the i~th` case together
with its controls.  Extensions of the logistic regression model that incor-
porate general relative risk functions have been proposed by Thomas (1981).
Let##(#.z). be the relative risk function with###a d-vector of parameters
to be estimated and z a d-vector representing covariate values.  Ordinary
logistic regression corresponds to##(#,z)#=#exp(#~t`z).  Whatever the form
of##(#,z), the appropriate conditional likelihood for matched case-control
studies is
.s2
where z`iq~ is the vector of covariates for the q~th` individual in risk set i,
and q#=#0 corresponds to the case.
.p5,1,1
For the purpose of diagnostics, it will be convenient to reformulate
the likelihood as follows.  Let Y`1~,Y`2~,...,Y`n~ be independent multinomial
random variables, each of sample size and such that probabilities associated
with Y`i~are (P`i0~,...,P`im`i~~) with#####P`iq~#=#0.  Without loss of generality,
assume that the observed 1 is associated with P##.  Define
.skip 2
Then, the likelihood 1 is the product of the P`i0~, which is the likelihood
of this multinomial realization.
.p5,1,1
For reasons that will become clear in section 4, it is useful to view L
in yet another way.  Let the P`is~ defined above be the expectation of
independent Poisson random variates.  Then, for this model, the likelihood
contribution from risk set R`i~ is
.indent 10
P`i0~ exp(-P`i0~)#####exp(-P`is~)#=#exp(-1)P`i0~.
Thus, up to a multiplicative constant, the likelihood arising from this
model is identical to L.
.skip
3.  The One-Step Newton-Raphson Approximation:
.p5,1,1
We briefly review Pregibon's approach (1981) to logistic regression
diagnostics.  Suppose now that Y`1~,Y`2~,...,Y`n~ are independent random variables
in the exponential family, and that the density Y`i~ is
.indent 15
g(y`i~,#`i~)#=#exp#y`i~#`i~#-#a(#`i~)#+#b(y`i~)#.
Then,##`i~ is the natural parameter of
Y`i~,a'(#`i~)#=#E(Y`i~) and a"(#`i~)#=#var(Y`i~).
For example, if Y`i~ is binomial,
then ##`i~#=#ln#~~#P`i_______________``1-P`i~~,
where P`i~ is the probability
of success.  Let##~t`#=#(#`1~,#`2~,...,#`n~) and suppose that###=#X#, where###is a
d-vector of parameters to be estimated, and X is the "design" matrix.
.p5,1,1
Differentiating the log likelihood with respect to###leads to the
following normal equations
.indent 15
X~t`##y-a'(#)##=X~t`r#=#0,
.br
where#####
{a'(#)}~t`#=#(~##a______```1~~#,~##a______```2~~#,#_._._.#,~##a______```n~~#).
.blank 1
The Newton-Raphson algorithm may be employed to solve these normal equations
for###and leads to the following iterative scheme.  If##~j` is the estimate at
j~th` iteration, then
.indent 15
#~j+1`#=##~j`#+#(X~t`V~j`X)~-1`x~t`r~j`,
where V~j` is the matrix of variances of the Y~i` evaluated at##~j`, and
r~j`#=##y-a'##(#~j`)###is the vector of residuals evaluated at the j~th`iteration.
.p5,1,1
Now let##~j`#=#X#~j`#+#(V~j`)~-1`r~j`.  Then, the above expression can be rewritten as
.indent 15
#~j+1`#=#(X~t`V#~j`X)~-1`X~t`V~j`#~j`,
and thus,##~j+1` can be interpreted to be the solution to a linear regression
problem with design matrix (V~j`)~1/2`X and observation vector (V~j`)~1/2`#~j`.  Similarly,
at convergence, the maximum likelihood estimate###is given by
.indent 15
##=#(X~t`VX)~-1`X~t`V##,
where now V is the matrix of variances at##, r#=##y-a'##(#)###, and
##=#X##+#V~-1`r.
.p5,1,1
As before, let##`(k)~ be the maximum likelihood estimate of###with the k~th`
observation deleted, V`-k~ the matrix V with the entry corresponding to the
k~th` observation deleted, X`-k~ the design matrix with the row corresponding
to the k~th`observation deleted, r`-k~ the vector r with the entry corresponding
deleted and##`-k~#=X`k~##+#V`-k~~_-1`r`-k~.  Now, with the k~th` observation deleted, and
with###as the initial value, suppose that one step of the Newton-Raphson
algorithm is taken towards#####, and let the one-step estimate be##`(k)~.  Then,
.indent 15
#####=#(X##V##X##)##X##V#####

is the solution to a linear expression problem; specifically, it is the
solution to the linear regression problem for###with the k###observation
deleted.  Hence, as in section l,

.center;60
##-######=#(X#VX)##x#v#r#/(1-h##),
where h###is the k###diagonal element of the "hat" matrix V#X(X#VX)##X#V#,
x##is the k###row of the design matrix X, and r##is the k###element of the
vector of pseudo-residuals r#=#V#X##=V#(#-X#)#=#V#(X##+#V##r#-#X#)#=#V##r.
The above expression for###-######is the Newton-Raphson one-step
approximation to####=###-#####.
.p5,1,1
The discussion above is easily generalized to the situation in which each
of the Y##is a vector valued random variable in the exponential family.
If Y##is in the m#-parameter exponential family, then####=(###,...,####) is
an m##vector of natural parameters.  Suppose that####=#(##,##,...,##) and
let###=#X#.  The appropriate approximation to####=###-#####, where######is
now the estimate of###on deleting the vector of observations corresponding
to Y#, is given by (Cook and Weisberg, p.l36).

.center;60
####(X#VX)###X#V##(I-H#)###V##r#,

V is now the block-diagonal covariance matrix of the Y##evaluated at##, V#
is the k###block of V, X##is the sub-matrix of X consisting of the rows
corresponding to Y#, and r## is the vector of residuals corresponding to Y#.



