                                                                 



1.Introduction


     The analysis   of   matched  case-control  studies  via  the  logistic

regression  model  and  extensions  of  it  has   become   commonplace   in

epidemiology.   These  analyses  raise  important  and  troubling questions

regarding the fit of the  statistical  models  and  the  stability  of  the

parameter  estimates.   More  precisely, we are interested in the following

questions.   First,  are  there  individual   observations   or   sets   of

observations   that  are  badly  fit  by  the  model?   Second,  are  there

observations that unduly influence the fit of the model in the  sense  that

deletion of these observations greatly influence the parameter estimates?


     In linear   regression,   these   questions   have   been  extensively

investigated. A crucial construction in linear  regression  diagnostics  is

that  of  the  hat matrix, and important concepts are those of residual and

                                                                       t   ~
leverage.  Consider the linear regression problem, y = X  +    where  y   =

                                                    t                      ~
(y ,y ,...,y )  is  an  n-vector  of observations,    = (  ,  ,...,  ) is a~
  1  2      n                                             1  2      d      

d-vector of parameters and X is an n x d design matrix.  Then, it  is  well

                                                 t  -1 t                   ~
known  that  the  vector of fitted values y = X(X X)  X Y.  Because of this

                                                t  -1 t                    ~
relationship between y and y, the matrix H = X(X X)  X  is called  the  hat

                th                                                         ~
matrix.   The  k    diagonal  element  of  H,  h  ,  is called the leverage~
                                                kk                         

                     th                            t  -1 t                 ~
associated with the k   observation y .  Let   = (X X)  X y be the estimate~
                                     k                                     

                                                           th              ~
of  , and suppose that      is the estimate of   with the k    observation,~
                        (k)                                                

y , deleted.  Then,~
 k                 

                                      t  -1            ~
                       =   -      = (X X)   x r /1-h  ,~
                     k        (k)            k k    kk 




                                                                 
                                                                     Page 2


                             th                                            ~
where r   =  y  - y  is the k   component of the vector of residuals and x ~
       k      k    k                                                       

is the row of X corresponding to the k   observation.  Thus, both  the  fit

as  measured  by  the  residual  r   and  the  leverage  are  important  in~
                                  k                                        

determining

  .  Cook (1977) proposed labelling points as influential if the  value  of~
 k                                                                         

the quadratic form

                         t   t                  2           2~
               (  -     )  (X X) (  -     ) = r  h  /(1-h  ) ~
                     (k)               (k)     k  kk     kk  

is unusually large.


     The main  goal  of this paper is to provide methods for evaluating the

impact of single observations (risk sets and individuals within risk  sets)

on the parameter estimates of regression models for the analysis of matched

case-control  data.   We  consider  both the logistic regression model, and

extensions of it, in  the  form  of  generalized  relative  risk  functions

(Thomas,  1981).   In  particular, we will be concerned with how individual

observations affect the choice of a model when the mixture model  suggested

by Thomas (1981) is used for the analysis.


     Now, let          represent  the  parameter  estimates  in  a  matched~
                 (k)                                                       

                                       th                                  ~
case-control study on deletion of the k   risk  set.   For  the  regression

models  used  in  case-control studies, exact formulae for   -      are not~
                                                                (k)        

available, and approximations must be employed.


     In section 2, we re-express the conditional likelihood  of  a  matched

case-   control   study  in  a  form  that  is  particularly  suitable  for

diagnostics.  In a recent paper, Pregibon (1981) has considered  diagnostic

statistics  in the context of logistic regression.  Pregibon's prescription

for approximating



                                                                 
                                                                     Page 3


  -      is based on a single iteration of  the  Newton-Raphson  algorithm,~
     (k)                                                                   

and  we review it in section 3 in the context of matched case-control data.

The one-step Newton-Raphson approximation is computationally  difficult  to

implement  with  generalized  relative  risk  and  mixture  models,  and we

introduce another, computationally simpler, approach  in  section  4.   Our

approach is identical to Pregibon's for logistic regression.  In section 4,

we also introduce the appropriate notions of hat matrix, residual, leverage

and influence.  Finally, in section 5, we present a detailed example.


2.  The Matched Case-Control Study:


                                                         th                ~
     Suppose that  there  are n cases, and that for the i   case, there are

m                                                                          ~
 i                                th                               th      ~
   controls.  Let R  denote the  i    risk  set,  that  is,  the  i    case~
                   i                                                       

together  with  its  controls.  Extensions of the logistic regression model

that incor- porate general relative risk functions have  been  proposed  by

Thomas (1981). Let  ( . z). be the relative risk function with   a d-vector

of  parameters  to  be  estimated  and  z a d-vector representing covariate

                                                                       t   ~
values.  Ordinary logistic regression corresponds to  ( , z)  =  exp(   z).

Whatever the  form  of   ( , z), the appropriate conditional likelihood for

matched case-control studies is



                                               th                          ~
where z   is the vector of covariates for the q   individual in risk set i,~
       iq                                                                  

and q = 0 corresponds to the case.


     For the purpose of diagnostics, it will be convenient  to  reformulate

the  likelihood  as  follows.   Let Y ,Y ,...,Y  be independent multinomial~
                                     1  2      n                           

random  variables,  each  of  sample  size  and  such  that   probabilities


`

                                                                 
                                                                     Page 4


associated  with  Y are  (P  ,...,P   )  with     P   = 0.  Without loss of~
                   i       i0      im              iq                      ~
                                     i                                     

generality, assume that the observed 1 is associated with P  .  Define



Then, the likelihood 1 is the product of the P  , which is  the  likelihood~
                                              i0                           

of this multinomial realization.


     For reasons  that will become clear in section 4, it is useful to view

L in yet another way.  Let the P   defined  above  be  the  expectation  of~
                                is                                         

independent  Poisson random variates.  Then, for this model, the likelihood

contribution from risk set R  is~
                            i   

                 P   exp(-P  )     exp(-P  ) = exp(-1)P  .~
                  i0       i0            is            i0 

Thus, up to a multiplicative constant, the  likelihood  arising  from  this

model is identical to L.


3.  The One-Step Newton-Raphson Approximation:


     We briefly  review  Pregibon's  approach (1981) to logistic regression

diagnostics.   Suppose  now  that  Y ,Y ,...,Y   are   independent   random~
                                    1  2      n                            

variables in the exponential family, and that the density Y  is~
                                                           i   

                   g(y ,  ) = exp y    - a(  ) + b(y ) .~
                      i  i         i i      i       i   

Then,      is  the  natural  parameter  of  Y ,a'(   ) = E(Y ) and a"(  ) =~
        i                                    i      i       i          i   

                                                        P                         ~
                                                         i____                    ~
var(Y ). For example, if Y  is binomial, then     = ln            ,  where  P   is~
     i                    i                     i             1-P            i    ~
                                                                 i                

                                    t                                      ~
the  probability  of success.  Let    = (  ,  ,...,  ) and suppose that   =~
                                          1  2      n                      

X , where   is a d-vector of parameters to  be  estimated,  and  X  is  the

"design" matrix.


     Differentiating the  log  likelihood  with  respect  to   leads to the
following normal equations



                                                                 
                                                                     Page 5


                           t             t      ~
                          X   y-a'( )  =X r = 0,

                  t      a__      a__               a__                          ~
where     {a'( )}   = (       ,        , _._._. ,         ).  The  Newton-Raphson~
                                                                                 ~
                            1        2                 n                         

algorithm  may  be employed to solve these normal equations for   and leads

                                        j                     th           ~
to the following iterative scheme.  If    is the estimate at j   iteration,

then

                         j+1    j     t j  -1 t j ~
                             =    + (X V X)  x r ,

       j                                    i               j        j     ~
where V  is the matrix of variances of the Y  evaluated at   , and  r   =  

        j                                                 th          ~
y-a'  (  )   is the vector of residuals evaluated at the j  iteration.


               j        j       j -1 j                                     ~
     Now let      =  X     +  (V )  r .   Then, the above expression can be

rewritten as

                          j+1     t  j  -1 t j j ~
                              = (X V  X)  X V   ,

           j+1                                                             ~
and thus,      can be interpreted to be the solution to a linear regression

                              j 1/2                               j 1/2  j ~
problem with design matrix  (V )   X  and  observation  vector  (V )      .

Similarly, at convergence, the maximum likelihood estimate   is given by

                                  t   -1 t    ~
                              = (X VX)  X V  ,

where now V is the matrix of variances at  , r =  y-a'  ( )   , and

          -1  ~
  = X  + V  r.


     As before,  let       be the maximum likelihood estimate of   with the~
                       (k)                                                 

 th                                                                        ~
k   observation deleted, V   the matrix V with the entry  corresponding  to~
                          -k                                               

       th                                                                  ~
the   k     observation  deleted,  X    the  design  matrix  with  the  row~
                                    -k                                     

                      th                                                   ~
corresponding to the k  observation deleted, r    the  vector  r  with  the~
                                              -k                           

                                                 _-1                     th~
entry  corresponding  deleted  and     =X   + V     r  .  Now, with the k  ~
                                    -k   k     -k    -k                    
observation deleted, and with   as the initial value, suppose that one step
of the Newton-Raphson algorithm is taken towards     , and let the one-step



                                                                 
                                                                     Page 6


estimate be     .  Then,~
             (k)        

                            = (X  V  X  )  X  V     

is the solution to a linear expression problem;  specifically,  it  is  the

solution  to  the  linear regression problem for   with the k   observation

deleted.  Hence, as in section 1,

                      -      = (X VX)  x v r /(1-h  ),

where h   is the k   diagonal element of the "hat" matrix V X(X VX)  X V  ,

x   is the k   row of the design matrix X, and r  is the k   element of the

vector of pseudo-residuals r = V X  =V ( -X ) = V (X  + V  r - X ) = V   r.

The   above   expression   for    -       is  the  Newton-Raphson  one-step

approximation to    =   -     .


     The discussion above is easily generalized to the situation  in  which

each  of  the  Y   is  a  vector  valued random variable in the exponential

family. If  Y   is  in  the  m  -parameter  exponential  family,  then     

=(   ,...,    )  is  an m  vector of natural parameters.  Suppose that    =

(  ,  ,...,  ) and let   = X .  The appropriate  approximation  to     =   

-     ,  where       is  now  the  estimate  of   on deleting the vector of

observations corresponding to Y , is given by (Cook and Weisberg, p.l36).

                         (X VX)   X V  (I-H )   V  r ,

V is now the block-diagonal covariance matrix of the Y  evaluated at   ,  V

is  the  k    block  of V, X  is the sub-matrix of X consisting of the rows

corresponding to Y , and r   is the vector of  residuals  corresponding  to

Y .










`

                                                                 

