

                                                                     Page 1


1.  MATCHED CASE-CONTROL AND SURVIVAL STUDIES



     Let n be the number of cases in a matched case-control study.  For the~

 th                                                                        ~
i    case,  there  are m  controls and these m  +l individuals together are~
                        i                     i                            
              th                                                           ~
known as the i   risk set.  In survival studies, we interpret n to  be  the

number  of  failures  and m  to be the number of individuals whose survival~
                           i                                               
                               th                                          ~
times are known to exceed the i   failure time.  We will assume  that  risk

sets  of  survival  analysis  are  ordered so that i = 1 corresponds to the

first failure and i = n corresponds to the last.  The risk sets of  matched

case-control  studies  are  disjoint, while one individual often appears in

more than one survival risk set.



     Let z   be a d-vector of covariate values collected from individual  g~
          ig                                                               ~

in  risk set i, g = 0,1,...,m , with g = 0 corresponding to the case on the~
                             i                                             ~

individual who failed.  The purpose of the data collection is to  test  the

null  hypothesis  that ܱb ܰ=  ܱb   ܰwhere ܱb ܰis  a d-vector of parameters in the~
                             0                                             ~

relative risk function ܱpܰ(ܱbܰ,z).  For example, the usual logistic  regression

in matched case-control studies and the original proportional hazards model~

                                 t                                         ~
of Cox (l972) had ܱpܰ(ܱbܰ,z) = exp( ܱb ܰz).  Our only interest will be in ܱb  ܰsuch~
                                                                     0     
                                                              t            ~
that ܱpܰ(ܱb ܰ,z  ) = ܱpܰ(ܱb ܰ,z  ), 0 < g, s < m . When ܱpܰ(ܱbܰ,z) = exp(ܱb ܰz), the null~~
                              _      _                                     ~
        0  ig       0  is               i                                  ~

hypothesis is ܱb ܰ= 0.



     The conditional   likelihood   of   the  logistic  model  for  matched

case-control studies is formally identical to the partial likelihood of the

proportional  hazards  model  of  survival  data.   In  either  case,   the

appropriate likelihood function (Thomas, 1981) is





                                                                     Page 2


                       ܱpܰ(ܱbܰ,z  )                                            ~
                  n         i0                                             ~
                     ____________                                          ~
          L(ܱbܰ) =  ܱ!                                                   ܰ(1.1)
                 i=1 m                                                     ~
                      i                                                    ~
                     ܱS  pܰ(ܱbܰ,z  )                                           ~
                             ig                                            ~
                    g=0                                                    ~

This  expression  suggests  defining  a  probability  of being a case or of

failing for each member of risk set i as


                   ܱpܰ(ܱbܰ,z  )~
                        is 
          P   = ------------  s = 0,1,...,m .                         (1.2)~
           is                              i                               
                 m          ~
                  i         ~
                 ܱS  pܰ(ܱbܰ,z  )~
                         ig ~
                g=0         
The null hypothesis ܱb ܰ= ܱb  ܰis then equivalent to the hypothesis~
                         0                                     
                             -1                                            ~
               P   = (m  + 1)    s = 0,1,...,m ; i = 1,...,n          (1.3)~
                is     i                      i                            ~

From (1.1) and (1.2) the log likelihood is~

                           n                                               
               log L(ܱbܰ) =  ܱS  ܰlog P                                   (1.4)~
                          i=1      i0                                      ~

Define the d-vector U and the (d x d) matrix V by~

                           n                                               ~
                              u (ܱbܰ)                                   (1.5)~
                   U(ܱbܰ) =  ܱS   ܰi                                           ~
                          i=1                                              
                           n                                               
and                V(ܱbܰ) =  ܱS  ܰv (ܱbܰ)                                   (1.6)~
                          i=1  i                                           
                      t                                                    ~
where          {u (ܱbܰ)}  = ܱMܰlog P  /ܱMb                                 ܰ(1.7)~
                 i              i0                                         
                           2          2                                    ~
and            v (ܱbܰ) = -E{ܱM ܰlog P  /ܱMb ܰ}                              (1.8)~
                i                i0                                        
    ^                                                                      ~
Let ܱb ܰbe the value of ܱb ܰthat maximizes (1.4).  Under  the  null  hypothesis

(1.3),  L(ܱb ܰ) = -E log(m  +1).   The generalized likelihood ratio statistic~
           0            i                                                  
                                              ^                         ~
measures the difference in log likelihoods at ܱb ܰand ܱb  ܰand is defined by~
                                                     0                  
                              ^              ~
               ܱg    ܰ= 2[log L(ܱbܰ) - log L(ܱb ܰ)]~
                GLR                       0  
                         n                                                 ~
                                       ^                                   ~
                    = 2  ܱS  ܰlog{m  + 1)P  }                           (1.9)~
                                 i      i0                                 ~
                        i=1                                                
      ^                                               ^                    ~
where P    is  calculated  from  (1.2)  evaluated  at ܱbܰ.   In  case-control~
       i0                                                                  ~

studies,   m   is  often  constant  so  that  ܱg    ܰis  large  wherever  the~
            i                                  GLR                         ~

probability of being a case is high for the individuals who were, in  fact,~

                                                                 ^         ~
the  cases.   In survival studies, m  is never constant, and the P   of the~
                                    i                             i0       


                                                                     Page 3


earlier (hence, larger) risk  sets  have  inherently  more  weight  in  the

calculation  of ܱg    ܰno  matter  what  choice is made for the relative risk~
                 GLR                                                       ~

function.



     The Wald statistic is a standardized measure of the difference between~

^                                       ~
ܱb ܰand ܱb ܰ.  Using (1.6), the statistic is~
       0                                
            ^      t    ^  -1  ^                                           ~
     ܱg   ܰ= {ܱb ܰ- ܱb ܰ}  {V(ܱbܰ)}   {ܱb ܰ- ܱb ܰ}                               (1.10)~
      ML         0                  0                                      ~

which is computationally much more difficult than (1.9) in case-control and~

                              ^                                            ~
survival studies.  However, V(ܱbܰ) is asymptotically  the  covariance  matrix~

   ^                                                                       ~
of ܱbܰ,  and the significance of individual elements of ܱb ܰcan be approximated

by~

                       ^       ~
                       ܱb  ܰ- ܱb  ~
                        ܰj    0j~

               t  = ----------  j=1,...,d                            (1.11)~
                j                                                          
                         ^      ~
                        (v  )ܱ1-2~
                          ܰjj    


      ^                   th             ^                           ^     ~
where ܱb  ܰand ܱb   ܰare the j   elements of ܱb ܰand ܱb ܰ, respectively, and v   is~
       j      0j                                0                     jj   
     th                       ^  ~
the j   diagonal element of V(ܱbܰ).



2.  A REPARAMETERIZATION~



                               ^                                           ~
     Both ܱg    ܰand ܱg   ܰrequire ܱb ܰfor the test statistic which means that an~
           GLR      ML                                                     ~

iterative  solution  is  necessary.   The  derivatives  of  (1.2)  can   be

inconvenient   to   program.    A   reparameterization   can  simplify  the

computations and provide insight into the methodology. To that end,  define~

                                 t   ~
the m -vector ܱr  ܰ= (ܱr  ܰ,...,ܱr ܰm )  by~
     i         i     i1      i i     ~

     ܱr   ܰ= log(P  /P  )     k = 1,...,m .                             (2.1)~
      ik        ik  i0                 i                                   ~

From (1.2), we have



                                                                     Page 4


     ܱr   ܰ= log{ܱpܰ(z  ,ܱbܰ) / ܱpܰ(z  ,ܱbܰ)}                                   (2.2)~
      ik          ik         i0                                            
                            t   ~
and, when     ܱpܰ(ܱbܰ,z) = exp(ܱb ܰz),~

            t                                                              ~
     ܱr   ܰ= ܱb ܰ(z   - z  .                                              (2.3)~
      ik       ik    i0                                                    ~

From  (2.2),  exp(ܱr  ܰ)  can  always  be  interpreted  as  a risk ratio with~
                   ik                                                      ~

exp(ܱr  ܰ) in (2.3) being a familiar expression of relative risk.~
     ik                                                        ~



     It is simple to manipulate (2.1) to reveal

               P   = 1 / {1 + ܱS ܰexp(ܱr  ܰ)}                             (2.4)~
                i0                   ik                                    ~

and            P   = exp(ܱr  ܰ) / {1 + ܱS ܰexp(ܱr  ܰ)} .~
                ik        ik                ik    ~

We can then use the chain rule, to re-write (1.7) as~

                  t                                                        ~
     u (ܱbܰ) = G (ܱbܰ)  s (ܱbܰ)                                             (2.5)~
      i       i      i                                                     ~

where          G (ܱbܰ) = ܱMr ܰ/ܱMb                                         ܰ(2.6)~
                i        i                                                 ~

and~

      t                                                                    ~
s ( ܱbܰ)  = ܱMܰlogP  /ܱMr  ܰ= (-P  ,...,-P   ).                             (2.7)~
 i             i0   i      11       im                                     ~
                                      i                                    ~

The (mixed) matrix G (ܱbܰ) is computable  directly  from  (2.2)  and  is  not~
                    i                                                      ~

dependent   on   the   likelihood  (1.4).   Rather,  G (ܱbܰ)  is  a  Jacobian~
                                                      i                    ~

relating ܱb ܰto ܱr ܰ.  On the other hand, s (ܱbܰ) represents  the  derivative  of~
               i                       i                                   ~

the  likelihood of risk set i with respect to the "new parameters" ܱr ܰ. When~
                                                                    i      
              t                                                        ~
ܱpܰ(ܱbܰ,z) = exp(ܱb ܰz), the derivative of (2.3) shows that row k in G (ܱbܰ) is~
                                                                i      
             t     t                                                       ~
     g   = z   - z      k = 1,...,m                                   (2.8)~
      ik    ik    i0               i                                       ~

and does not depend on ܱb ܰ.



     By differentiating  (2.5)  and  taking  expectations,  (1.8)  can   be

re-written as~

                  t                                                        ~
     v (ܱbܰ) = G (ܱbܰ)  w (ܱbܰ) G (ܱbܰ)                                       (2.9)~
      i       i      i     i                                               ~

where~

                                    t                                      ~
     w (ܱbܰ) = diag(P  ) - s (ܱbܰ) s (ܱbܰ)                                 (2.10)~
      i            ik     i     i                                          


                                                                     Page 5


and   diag(P  )   is   an   (m  x m )   diagonal   matrix   with   elements~
            ik                i    i                                       ~

P    (k = 1,...,m ).~
 ik              i  ~



     Let N = ܱS ܰm  and define the N-vector s(ܱbܰ) and the (N x d) matrix  G(ܱbܰ)~
                i                                                          ~

as~

                  t          t t                                           ~
     s(ܱbܰ) = {s (ܱbܰ) ,...,s (ܱbܰ) }                                      (2.11)~
              1          n                                                 
                            t          t t                                 ~
and            G(ܱbܰ) = {G (ܱbܰ) ,...,G (ܱbܰ) }                            (2.12)~
                        1          n                                       
                                                                         th~
Similarly, define the (N x N) matrix W(ܱbܰ) to be block diagonal with the i  

block  being  the  (m  x m )  matrix  w (ܱbܰ).   Then, (1.5) and (1.6) can be~
                     i    i            i                                   ~

re-written as~

          t                                                                ~
     U = G s                                                         (2.13)~

                    t                                                      ~
and            V = G WG                                              (2.14)

where, for notational convenience, we have suppressed the dependence of all~

                                                      ^                    ~
quantities on ܱbܰ.  Finally, the iterative solution for ܱb  ܰcan be found using

the Newton-Raphson procedure~

               ^j+1   ^j    ^j -1 ^j                                       ~
               ܱb    ܰ= ܱb  ܰ+ {V }   U                                  (2.15)~

      ^     ^j                                   ^j                        ~
where V and U  are (2.3) and (2.14) evaluated at ܱb ܰ, the  estimate  of ܱb ܰat

iteration j.  One can recognizethat (2.15) is equivalent to iterative least

squares  with  an  (N  x  d)  "design matrix" X(ܱbܰ) and (N x L) "observation

vector" Y(ܱbܰ) defined by~

                 1/2                                                       ~
     X(ܱbܰ) = {W(ܱbܰ}    G(ܱbܰ)                                            (2.16)~

                           -1/2                                            ~
and            Y(ܱbܰ) ={W(ܱbܰ)}     s(ܱbܰ)                                 (2.17)

Thus, an iterative least squares computer program can easily be adapted  to

provide maximum likelihood estimates of ܱb  ܰfor any ܱpܰ(ܱbܰ,z).~



                                         t                                 ~
     As we have seen, when ܱpܰ(ܱbܰ,z) = exp(ܱb ܰz), the "design matrix" X depends

on  ܱb  ܰonly  through  the  "weight matrix" W.  The observation vector Y(ܱbܰ),

however, always depends on ܱb ܰthrough both W and s. In fact, an  alternative


                                                                     Page 6


characterization  of  maximum  likelihood  estimation  for case-control and~

                         ^                      ^ t   ^                    ~
survival studies is that ܱb ܰis the solution to X(ܱbܰ)  Y(ܱbܰ) =0.  This  can  be

re-expressed by defining the regression sum of squares, RSS, as~

               t   t  -1 t                                                 ~
     RSS(ܱbܰ) = Y X(X X)  X Y                                          (2.18)~

                                                                 ^         ~
where  the  dependence of Y and X on ܱb ܰis suppressed.  Then, RSS(ܱbܰ) =0.  We

will pursue this line of reasoning further after a numerical illustration.



3.  LIKELIHOOD CONTOURS



     Thomas (1980) advocated a mixture  model  for  the  relative  function

defined by~

                     t  1-ܱa       ܰt   1a                              (3.1)~
     ܱpܰ(ܱbܰ,z) = {(1 + ܱb ܰz}    {exp(ܱb ܰz)}                                     

When  ܱa ܰ=1; the model is the familiar one that we have used as an example a

number of times thus far.  This model is also known as  the  multiplicative

model  and  provides  a  contrast  to the ܱa ܰ= 0 model known as the additive

model.



     Breslow and  Day  (1980,  Appendix  III)  have  presented  a   matched

case-control  study of endometrial cancer in Los Angeles.  There are n = 63

risk sets  with  m  = 4  controls  in  each.   We  have  chosen  the  d = 2~
                  i                                                        ~

covariates  history  of gall bladder disease and length of estrogen use for

analysis.  We will denote the log likelihood as log L(ܱaܰ,ܱbܰ,ܱb ܰ,ܱb ܰ)  in  order~
                                                           1  2            ~

to include the dependence of ܱpܰ(ܱbܰ,z) on ܱaܰ.~



                                                         ^ ^  ^            ~
     Maximizing over  all  three unknowns, we find log L(ܱaܰ,ܱb ܰ,ܱb ܰ) = -82.94.~
                                                            1  2           ~

If  X  is  fixed  and  just  ܱb   ܰand  ܱb   ܰare  allowed  to  vary,  we  find~
                              1        2                                   
        ^  ^                       ^  ^                                    ~
log L(0,ܱb ܰ,ܱb ܰ) = -83.14.   log L(1,ܱb ܰ,ܱb ܰ) = -85.45.   On  this  basis,  the~
         1  2                       1  2                                   ~

hypothesis ܱa ܰ= 1 can be rejected at the .05  level  using  the  generalized


                                                                     Page 7


likelihood  ratio  test.   Thus, the additive model ܱa ܰ> 0) seems to be more~

                                    ^        ~
appropriate for the data.  In fact, ܱa ܰ= -.04.



     Under the null  hypothesis ܱb ܰ= 0,  all  P   = 1/s  in  (1.3)  so  that~
                                              is                           ~

log L(ܱaܰ,0,0) = -63 log 5 = -101.39.   Hence,  ܱg    ܰ= 36.50 for the additive~
                                               GLR                         ~

model and resounding rejects the null  hypothesis.   Note  that ܱg    ܰ= 36.9~
                                                                 GLR       
      ^                                                                    ~
when  ܱa ܰ= -1.04  and ܱg    ܰ= 31.88  when ܱa ܰ= 1 so that ܱb ܰ= 0 is consistently~
                      GLR                                                  ~

untenable.



     On computing (1.10), we found that at ܱa ܰ= 0, ܱg   ܰ= 5.48, at  ܱa ܰ= -.04,~
                                                   ML                      ~

ܱg   ܰ= 2.76,  and ܱa ܰ= 1, ܱg   ܰ= 27.32. This means ܱg   ܰis not even significant~
 ML                      ML                      ML                        ~

at the .05 level for the additive model  or  with  ܱa  ܰset  to  its  maximum

likelihood  estimate.  Similarly,  the  statistics  (t ,t ) from (1.11) are~
                                                      1  2                 ~

(1.61, 2.16) at ܱa ܰ+ 0, (1.19, 1.55) at ܱa ܰ= -.04, and (2.87, 4.36) at ܱa ܰ= 1.

From these rather surprising  results,  we  surmized  that  the  asymptotic~

                ^  upon  which the significance of ܱgܰML and (t ,t ) is based~
properties  of  ܱb                                            ܰ1  2          

probably do not hold for ܱa ܰ= 0 or ܱa ܰ= .04.~



                                                               ^  ^        ~
     To verify this assertion, we plotted the locus of points (ܱb ܰ,ܱb ܰ)  such~
                                                                1  2       
                 ^  ^                                                      ~
that  2{log  L(ܱaܰ,ܱb ܰ,ܱb             ܰ^  ^                                     ~
                  1  2) - log L(ܱaܰ,ܱb ܰ,ܱb ܰ)} = 3.84.   Figure  1 displays this~
                                   1  2                                    
                                                           ^  ^            ~
contour with ܱa ܰ= 0.  The "X" marks  the  coordinates  of  (ܱb ܰ,ܱb ܰ)  and  all~
                                                            1  2           ~

points  within  the  region constitute the 95% connfidence set for (ܱb ܰ,ܱb ܰ).~
                                                                     1  2  
                                ^                                          ~
If the asymptotic properties of ܱb ܰheld for these data, this region would be

elliptical.  It is clear than that there is a serious problem  in  applying

the asymptotic theory here.~



                                                                          ^~
     Figure 2  displays  this  score contour with X = -.04.  Setting ܱa ܰto ܱa
ܰ

                                                                     Page 8


makes the confidence region have an absolutely dreadful shape. With  ܱa ܰ= 1,

things are much improved as can be seen in Figure 3.



     Even though  the  likelihood  at  ܱa ܰ= 1 is significantly lower than at~

    ^                                                                ^     ~
ܱa ܰ= ܱaܰ, tests of ܱb ܰ= 0 based on the asymptotic distribution theory of ܱb  ܰare~

                               ^                                           ~
viable at ܱa ܰ= 1 but not at ܱa ܰ= ܱaܰ.  One way to avoid this paradox is not use~

^                                                                          ~
ܱb  ܰat  all  to  test  ܱb ܰ= 0.   We  discuss this in the next section, but we

re-emphasize here that many "stepwise" procedures to "covariate  selection"

depend  upon various functions of the t-statistics in (1.11).  The validity

of these procedures requires careful monitoring.



4.  SCORE TEST



     Rao (1973, p.  ) defines a third test statistic for ܱb ܰ= ܱb   ܰthat  does~
                                                              0            
                                        ^                                  ~
not  depend  upon  the  calculations of ܱbܰ.  This test is known as the score

test and is given~

                 t        -1                                               ~
     ܱg  ܰ= {U(ܱb ܰ)}  {V(ܱb ܰ)}   U(ܱb ܰ).                                   (4.1)~
      S       0        0        0                                          ~

where U(ܱb ܰ) and V(ܱb ܰ) are  (1.5)  and  (1.6)  evaluated  at  ܱb ܰ= ܱb ܰ.   From~
         0         0                                              0        
                                                                    t      ~
(2.13),  (2.14),  (2.15), and (2.16), it is clear that U(ܱb ܰ) = X(ܱb ܰ)  Y(ܱb ܰ)~
                                                          0       0      0 
                 t                                                         ~
and V(ܱb ܰ) = X(ܱb ܰ)  X(ܱb ܰ).  Thus, from (2.18)  ܱg  ܰ= RSS(ܱb ܰ).   As  discussed~
       0       0      0                        S        0                  
              ^                                                            ~
earlier,  RSS(ܱbܰ) = 0  and,  therefore,  ܱg   ܰis another measure of deviation~
                                         S                                 
        ^                                                                  ~
between ܱb ܰand ܱb  ܰin the same spirit as ܱg     ܰand  ܱg  ܰ.  This  provides  the~
               0                        GLR        ML                      ~

intuitive  motivation for ܱg  ܰin case-control and survival studies while the~
                           S                                               
                                            ^               ~
computational desirability is clear because ܱb ܰis not needed.



     Since (1.3) is equivalent to ܱb ܰ= ܱb ܰ, (2.7) and (2.10) evaluated  under~
                                       0                                   ~

the null hypothesis are



                                                                     Page 9


                       -1                                                  ~
     s (ܱb ܰ) = -(m  + 1)   l                                           (4.2)~
      i  0       i         i                                               
                      -1             -2    t                               ~
and  w (ܱb ܰ) = (m  + 1)   I - (m  + 1)   l l                           (4.3)~
      i  0      i              i         i i                               ~

where  l   is  a vector of length m  with every element equal to l and I is~
        i                          i                                       ~

the identity matrix (of order m ).  It is then  straightforward  to  verify~
                               i                                           ~

that~

             -ܱ1-2    ܰ-1             -1    t                                ~
     {w (ܱb ܰ)}     = a   I + (a  + 1)   l l                            (4.4)~
       i  0           i       i         i i                                
                   -ܱ1-2                                            ~
ܰwhere a  = (m  + 1)    .  We can then define the m  vector of y  as~
       i     i                                    i            i   
                 -ܱ1-2                                                      ~
     ܰy  = {w (ܱb ܰ}     s (ܱb ܰ)                                          (4.5)~
      i     i  0       i  0                                                ~

and find the surprising result that

     y  = -l                                                          (4.6)~
      i     i                                                              ~

From  (2.17),  it can be seen that the elements of y  are the rows of Y(ܱb ܰ)~
                                                    i                    0 ~

that correspond to risk set i.  Thus, every element  in  the  vector  Y(ܱb ܰ)~
                                                                         0 ~

equals -1.  We then have

PROPOSITION  1:  The score statistic ܱg  ܰin (4.1) can be computed by summing~
                                      S                                    
                                                 t  -1  t                  ~
all the elements in the  (N x N)  matrix  H = X(X X)   X   where  X = X(ܱb ܰ)~
                                                                         0 ~

defined by (2.16) evaluated at ܱb ܰ= ܱb ܰ.~
                                    0 ~



     Each row in H corresponds to one individual and thus the row sums of H

provide  a  measure of the contribution of each individual to ܱg ܰ. The total~
                                                               S           ~

of the row sums for all of the individuals  in  risk  set  i  measures  the

contribution  of  the  entire  risk  set  to  the score statistic. Since in

survival analysis one person appears in many risk sets, it is  possible  to

locatew  the  appropriate  rows  of H to measure the overall impact of that

person.   Time  dependent  covariates  present  no  difficulties  in   this

formulation  so  that a great variety of interesting summary statistics are

available.  These calculations are  certain  to  be  more  insightful  than

simply presenting ܱg  ܰalone.  We will have more to present about "designated~
                   S                                                       


                                                                    Page 10


statistics" in the next section.



     Unfortunately, proposition  1  does  not provide as efficient a method

for calculating ܱg  ܰsince H is a large matrix.  The computation of  ܱg   ܰwill~
                 S                                                  S      ~

be facilitated by noting that from (2.5) and (4.6)~

                                t                                          ~
                       -1 G (ܱb ܰ)  l .                                 (4.7)~
     u (ܱb ܰ) = -(m  + 1)    i  0    i                                       ~
      i  0       i                                                         
                   t                                                       ~
When ܱpܰ(ܱbܰ,z) = exp(ܱb ܰz), G (ܱbܰ) does not depend on ܱb ܰand (2.8) can be used to~
                         i                                                 ~

show that

u (ܱb ܰ) = - ܱS  ܰ(z   -z  ) / (m  + 1)                                   (4.8)~
 i  0           ik   i0      i                                             ~

This  means  that  u (ܱb ܰ)  contains  the  "average"  difference between the~
                    i  0                                                   ~

covariate values of the controls and  the  case.   As  before  in  survival

analysis,  the case is the individual who failed and the controls are those

who survived longer than the case.  Clearly, if u (ܱb ܰ) = 0, then risk set i~
                                                 i  0                      ~

has  nothing  to  contribute  to  U(ܱb ܰ)  and  hence,  to  ܱg ܰ.   This  is  a~
                                     0                     S               ~

generalization  of  a  familiar  result in matched pairs (m  = 1) when only~
                                                           i               ~

discordant pairs contribute  useful  data.   A  systematic  examination  of

u (ܱb ܰ) will certainly provide additional interesting summary information in~
 i  0                                                                      ~

both case-control and survival analysis.



     Returning to the mixture model (3.1), we have~

                           t            t             t                    ~
     ܱM ܰlog ܱpܰ(ܱbܰ,z) / ܱMb ܰ= ܱaܰz  / log(1 + ܱb ܰz) + (1 - ܱaܰ)z                (4.9)~

                                                                          t~
Evaluating at the null hypothesis ܱb ܰ= 0, we can see that (4.9) equals to z 

for  any  ܱaܰ.   The rows of G (ܱb ܰ) for the mixture model are always given by~
                            i  0                                           ~

g   in (2.8).  Hence, the interpretation of  u (ܱb ܰ)  given  in  (4.8)  that~
 ik                                           i  0                         
                                      t                                    ~
seemingly depended upon ܱpܰ(ܱbܰ,z) = exp(ܱb ܰz) is, in fact, very general for the

mixture model.  We also have

PROPOSITION 2:  The score test for the null hypothesis ܱb ܰ= 0 in the mixture


                                                                    Page 11


model (3.1) does not depend on ܱaܰ.

PROOF:   From  (4.3),  W(ܱb ܰ)  does  not  depend on ܱa ܰso that X(ܱb ܰ) does not~
                          0                                     0          ~

depend on ܱaܰ.  The result follows from Proposition l.



     Day and Byar (1978)  have  shown  that  the  score  test  ofܱb ܰ= 0  for~

              t                                                            ~
ܱgܰ(ܱbܰ,z) = exp(ܱb ܰz)   in  the  logistic  model  of  case-control  studies  is

equivalent to the Mantel-Haenszel test.  Proposition 2 permits an extension

of that equivalence to the  mixture  model.   For  the  endometrial  cancer

discussed  previously,  ܱg  ܰ= 34.62  which  agrees  quite well with ܱg    ܰand~
                         S                                          GLR    ~

withܱg   ܰwith ܱa ܰ= 1.~
     ML            ~



5.  DIAGNOSTIC STATISTICS



     As discussed previously, examination of u (ܱb ܰ) is likely  to  be  very~
                                              i  0                         ~

revealing  in understanding how risk set i affects the conclusions.  If the

number of covariates is large, it would be useful  to  have  an  expression

which would measure the change in ܱg  ܰupon deletion of a risk set.~
                                   S                             ~



     Define C    as  the sum of all the entries in the row of H (defined in~
             ik                                                            ~

Proposition 1) that corresponds to individual k in risk set i, k = 1,...,m ~
                                                                          i~

and  i = 1,...,n.   Recall  that  (2.8)  gives  the  row  of  G (ܱb ܰ)   that~
                                                               i  0        ~

corresponds  to  this person in the mixture model (3.1).  Expression (2.16)

and (4.3) then define the appropriate row of X(ܱb ܰ) which, in turn,  defines~
                                                0                          ~

the  row of H.  As before, we will interpret C  = ܱS ܰC   as the contribution~
                                              i      ik                    ~

of risk set i to ܱg  ܰbecause ܱg  ܰ= ܱS ܰC  by Proposition  1.   From  (1.5)  and~
                  S          S      i                                      ~

(4.1), we have~

                 t        -1       ~
     ܱg  ܰ= ܱS ܰU(ܱb ܰ)  {V(ܱb ܰ)}   u (ܱb ܰ)~
      S        0       0      i  0 ~

from which it follows that


                                                                    Page 12~


           t                                                               ~
     C  = K  u (ܱb ܰ)                                                   (5.1)~
      i       i  0                                                         
                              t      -1                                    ~
where the d-vector K = {V(ܱb ܰ)}  U(ܱb ܰ)  .  Thus, C  is simply a weighted sum~
                           0       0             i                         ~

of the elements of u (ܱb ܰ), and H is not needed for the calculations.~
                    i  0                                            ~



     Define ܱg ܰ(-i)  as  the actual value of (4.1) upon deletion of risk set~
             S                                                             ~

(i) and let

     ܱW  ܰ= ܱg  ܰ- ܱg  ܰ(-i)                                                (5.2)~
      i    S    S                                                          ~

We have

APPROXIMATION 1:  An approximation to ܱW  ܰin (5.2) is given by  A   = C   in~
                                       i                        ii    i    ~

(5.1).



     As discussed  earlier, ܱg  ܰ= RSS(ܱb ܰ).  This permits an exact expression~
                             S        0                                    ~

to be given for ܱW ܰ.  Let x  be an (m  x d) matrix whose rows are  the  rows~
                 i        i         i                                      ~

of X(ܱb ܰ) corresponding to risk set i and define~
      0                                        
                   -1  t ~
     H  = x {V(ܱb ܰ)}   x .~
      i    i    0      i ~

Recall that y  in (4.5) is the vector of elements of Y(ܱb ܰ) corresponding to~
             i                                          0                  ~

risk  set  i  and  define  e   to  be  a vector of length m  whose elements~
                            i                              i               ~

correspond to risk set i  in  the  vector  e = Y(ܱb ܰ) - H Y(ܱb ܰ),  where,  as~
                                                  0         0              
                         -1        t                                       ~
before, H = X(ܱb ܰ) {V(ܱb ܰ)}   {X(ܱb ܰ)} .  Then, by a standard result (Cook and~
               0      0         0                                          ~

Weisberg, 1982, p.l36)~

           t       t         -1                                            ~
     ܱW  ܰ= y  y  - e  (I - H )   e .                                   (5.3)~
      i    i  i    i       i     i                                         
                                     t                                     ~
From   (4.6),   y  = -l   so  that  y  y  = m   and  element  k  of  e   is~
                 i     i             i  i    i                        i    ~

e   = -1 + C  .  These simplifications are  useful,  but,  the  inverse  of~
 ik         ik                                                             ~

(I - H )  is  a  serious  computational  problem.  Hence,  H  is an (N x  )~
      i                                                                    ~

projection matrix of rank d which is usually  substantially  less  than  N.~

                                                          2                ~
The  submatrix  H  is, therefore, likely to be small and H  is nearly zero.~
                 i                                        i                
                                                                   -1      ~
This suggests that I + H  is a suitable approximation  to  (I - H )  .   We~
                        i                                        i         ~

now have


                                                                    Page 13


APPROXIMATION   2:    An   approximation   to  ܱW   ܰin  (5.3)  is  given  by~
                                                i                          
            t            ~
A   = m  - e  (I +H ) e .~
 i2    i    i      i   i ~



     In survival analysis, even H is likely to be a fairly large matrix for

the earlier risk sets, and one can consider using I rather than (I + H ) as~
                                                                      i    ~

the approximate inverse of (I - H ).  Then,~
                                 i         ~

APPROXIMATION 3:  An approximation to ܱW  ܰis given by~
                                       i            
            t                2.~
A   = m  - e  e  = 2C  - ܱS ܰR   ~
 i3    i    i  i     i      ik ~



     One is tempted to dismiss A   as being overly economical in  terms  of~
                                i3                                         ~

computation.  However, A   has a surprising relationship with ܱg ܰ.~
                        i3                                     S ~

PROPOSITION 3:     ܱg  ܰ= ܱS ܰA  ~
                    S      i3~

PROOF:   Let  C  be  an  N-vector  whose  elements  are the C     k, i.  By~
                                                             ik            ~

definition, C = HL where L is an N-vector with every element  equal  to  l.

Then,~

 t     t t      t                                 ~
C C = L H HL = L HL = ܱg  ܰby proposition 1.  Hence,~
                       S                          
                        2                 ~
lS A   = 2 ܱS ܰC  - ܱS S ܰC   = 2ܱg  ܰ- ܱg  ܰ= ܱg ܰ.~
    i3        i        ik     S    S    S ~



     We used  a  subset of the heart transplant data (Crowley and Hu, 1977)

to evaluate these approximations.  Only the  65  individuals  who  actually

received  a  transplant  were considered.  Of these, n = 29 died because of

graft rejection and we chose d = 3 covariates - age at  transplant  (z   in~
                                                                      4    ~

Crowley  and Hu), mismatch score (z ) and HLA-A2 antigen indicator (z ) for~
                                   9                                 8     ~

analysis.  The hypothesis ܱb ܰ= 0 in (3.1) is rejected because ܱg  ܰ= 19.92.~
                                                              S         ~



     The approximation A   was nearly identical to  ܱW   ܰfor  all  the  risk~
                        i2                           i                     ~

sets.  This was not too surprising since H is a 996 x 996 matrix of rank 3.



                                                                    Page 14


Figure 4 shows that A   is much better than A  .~
                     i3                      i1 ~



     Risk sets  1  and  13  make  a large contribution to ܱg ܰ.  On the other~
                                                           S               ~

hand,  the  deletion  of  either  risk  set  5  or  22  will  increase   ܱg ~
                                                                          ܰS~

substantially.   The  reason for these changes can be understood by letting

the elements of u (ܱb ܰ) be given by u  , u   and u  .  From  (4.8),  we  can~
                 i  0               ii   i2      i3                        ~

interpret  u    as the average difference in age between the individual who~
            ii                                                             ~

died and  those  who  survived  longer.   Similarly,  u    is  the  average~
                                                       i2                  ~

difference  in mismatch score.  Figure 5 displays u   and u   for each risk~
                                                   ii      i2              ~

set. The preponderance of values in the first quadrant indicate that  those

who  died  tended  to be older and have a higher mismatch score.  Note that

risk sets 1 and 13 are the most extreme examples of  this  general  pattern

and,  hence, their major contribution to ܱg  ܰfollows directly.  On the other~
                                          S                                ~

hand, the individuals who died in risk sets 5 and 22 tended to  be  younger

and  better matched than their "controls." Thus, the deletion of these risk

sets increases ܱg ܰ.~
                S ~



     Every risk set in  which  the  individual  who  died  had  the  HLA-A2

indicator  present,  also had u   < 0 and u   < 0 (fig. 5). Therefore, this~
                               ii          i2                              ~

covariate interacts with the other two and is unlikely to be a  significant

predictor of survival.

