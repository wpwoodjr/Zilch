.rm 75
.ls 2
.sp 2
1.  MATCHED CASE-CONTROL AND SURVIVAL STUDIES
.sp 1
.ti +5
Let n be the number of cases in a matched case-control study.  For the
i~th` case, there are m`i~ controls and these m`i~ individuals together
are known as the i~th` risk set.  In survival studies, we interpret n to
be the number of failures and m`i~ to be the number of individuals whose
survival times are known to exceed the i~th` failure time.  We will assume
that risk sets of survival analysis are ordered so that i = 1 corresponds
to the first failure and i = n corresponds to the last.  The risk sets of
matched case-control studies are disjoint, while one individual often
appears in more than one survival risk set.
.sp 1
.ti +5
Let z`ig~ be a d-vector of covariate values collected from individual g in
risk set i, g#=#0,1,...,m`i~, with g#=#0 corresponding to the case on the
individual who failed.  The purpose of the data collection is to test the
null hypothesis that###=##`0~ where###is a d-vector of parameters in the
relative risk function##(#,z).  For example, the usual logistic regression
in matched case-control studies and the original proportional hazards
model of Cox (l972) had##(#,z)#=#exp(#~t`z).  Our only interest will be
in##`0~ such that##(#`0~,z`ig~)#=##(#`0~,z`is~), 0#<_#g, s#<_#m`i~.
When##(#,z)#=#exp(#~t`z), the null hypothesis is###=#0.
.sp 1
.ti +5
The conditional likelihood of the logistic model for matched case-control
studies is formally identical to the partial likelihood of the
proportional hazards model of survival data.  In either case, the
appropriate likelihood function (Thomas, 1981) is
.ls 1
.sp 1
.indent 23
#(#,z`i0~)
.br
.i 10
L(#) = ### ------------
.rj
(1.1)
.br
.i 21
`g=0~##(#,z`ig~)
.br
.ls2
.b
This expression suggests defining a probability of being a case or of
failing for each member of risk set i as
.ls 1
.sp 1
.indent 19
#(#,z`i0~)
.br
.i 10
P`is~ = ------------  s=0,1,...,m`i~.
.rj
(1.2)
.br
.i 17
`g=0~##(#,z`ig~)
.br
.ls2
.b
The null hypothesis###=##`0~ is then equivalent to the hypothesis
.indent 15
P`is~#=#(m`i~#+#1)~-1`##s#=#0,1,...,m`i~; i#=#1,...,n
.rj
(1.3)
.Br
From (1.1) and (1.2) the log likelihood is
.indent 15
log L(#)#=#####log#P`i0~
.rj
(1.4)
.br
Define the d-vector U and the (d x d) matrix V by
.indent 15
U(#)#=#####u`i~(#)
.rj
(1.5)
.br
and############V(#)#=#####v`i~(#)
.rj
(1.6)
.br
where##########{u`i~(#)}~t`#=##log#P`i0~/
.rj
(1.7)
.br
and############v`i~(#)#=#-E{#~2`log#P`i0~/##~2`}
.rj
(1.8)
.br
Let##~^`#be the value of###that maximizes (1.4).  Under the null hypothesis
(1.3), L(#`0~)#=#-E#log(m`i~#+1).  The generalized likelihood ratio
statistic measures the difference in log likelihoods at##~^`#and##`0~
and is defined by
.indent 15
#`GLR~#=#2[log#L(##~^`)#-#log#L(#`0~)]
.indent 20
=#2#####log{m`i~#+#1)P~^``i0~}
.rj
(1.9)
.br
where P~^``i0~ is calculated from (1.2) evaluated at##~^`.  In case-control
studies, m`i~ is often constant so that #`GLR~#is large wherever the
probability of being a case is high for the individuals who were, in fact,
the cases.  In survival studies, m`i~ is never constant, and the P~^``i0~
of the earlier (hence, larger) risk sets have inherently more weight in
the calculation of##`GLR~#no matter what choice is made for the relative
risk function.
.sp 1
.ti +5
The Wald statistic is a standardized measure of the difference between
#~^`#and##`0~.  Using (1.6), the statistic is
.indent 15
#`ML~#=#{#~^`#-##`0~}~t`#{V(#~^`)}~-1#{#~^`#-##`0~}
.rj
(1.10)
.br
which is computationally much more difficult than (1.9) in case-control
and survival studies.  However, V(#~^`) is asymptotically the
covariance matrix of##~^`, and the significance of individual
elements of###can be approximated by
.sp 2
where##~^``j~ and##`0j~ are the j~th` elements of##~^`#and##`0~,
respectively, and#v~^``jj~#the j~th` diagonal element of V(#~^`).
.sp 1
2.  A REPARAMETERIZATION
.p
Both##`GLR~ and##`ML~ require##~^`#for the test statistic which
means that an iterative solution is necessary.  The derivatives of
(1.2) can be inconvenient to program.  A reparameterization can
simplify the computations and provide insight into the methodology.
To that end, define the m`i~-vector##`i~#=#(0`i1~,...,#`i~m`i~)~t` by
.indent 10
#`ik~#=#log(P`ik~/P`i0~)#####k#=#1,...,m`i~.####################(2.1)
.br
From (1.2), we have
.indent 10
#`ik~#=#log{#(z`ik~,#)#/##(z`i0~,#)}############################(2,2)
.br
and, when######(#,z)#=#exp(#~t`z),
.indent 15
#`ik~#=##~t`(z`ik~#-#z`i0~.##################################(2.3)
From (2.2), exp(#`ik~) can always be interpreted as a risk ratio with
exp(#`ik~) in (2.3) being a familiar expression of relative risk.
.p
It is simple to manipulate (2.1) to reveal
.indent 15
P`i0~#=#1#/#{1#+###exp(#`ik~)}##################################(2.4)
.br
and############P`ik~#=#exp(#`ik~)#/#{1#+###exp(#`ik~)}
.br
We can then use the chain rule, to re-write (1.7) as
.indent 15
u`i~(#)#=#G`i~(#)~t`#s`i~(#)####################################(2.5)
.br
where##########G`i~(#)#=###`i~/#################################(2.6)
.br
and#####s`i~(#)~t`#=##logP`i0/##`i~#=#(-P`11,...,-P`im`i~~).####(2.7)
.br
The (mixed) matrix G`i~(#) is computable directly from (2.2) and is
not dependent on the likelihood (1.4).  Rather, G`i~(#) is a Jacobian
relating###to##`i~.  On the other hand, s`i~(#) represents the
derivative of the likelihood of risk set i with respect to##`i~.
When #(#,z)#=#exp(#~t`z), the derivative of (2.3) shows that row k
in G`i~(#) is
.indent 15
g`ik~#=#z`ik~~t`#-#z`i0~~t`####k#=#1,...,m`i~#############(2.8)
.br
and does not depend on###.
.p
By differentiating (2.5) and taking expectations, (1.8) can be
re-written as
.indent 15
v`i~(#)#=#G`i~(#)~t`#w`i~(#)#G`i~(#)############################(2.9)
.br
where
.indent 15
w`i~(#)#=#diag(P`ik~)#-#s`i~(#)#s`i~(#)~t`######################(2.10)
.br
and diag(P`ik~) is an (m`i~#x#m`i~) diagonal matrix with elements
P`ik~##(k#=#1,...,m`i~).
.p
Let N#=##m`i~ and define the N-vector s(#) and the (N#x#d) matrix
G(#) as
.indent 15
s(#)#=#{s`1~(#)~t`,...,s`n~(#)~t`}~t`###########################(2.11)
.br
and############G(#)#=#{G`1~(#)~t`,...,G`n~(#)~t`}~t`############(2.12)
.br
Similarly, define the (N#xN) matrix W(#) to be block diagonal with
the i~th` block being the (m`i~#x#m`i~) matrix w`i~(#).  Then, (1.5)
and (1.6) can be re-written as
.indent 15
U#=#G~t`s#######################################################(2.13)
.br
and############V#=#G~t`wG#######################################(2.14)
.br
where, for notational convenience, we have suppressed the dependence
of all quantities on##.  Finally, the iterative solution for##~^`#
can be found using the Newton-Raphson procedure
.indent 15
#~^`~j+1#=##~^`~j`#+#{V~^`~j`}~-1`#U~^`~j`##########(2.15)
.br
where 


